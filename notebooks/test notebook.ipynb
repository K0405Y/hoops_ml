{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from databricks.sdk import WorkspaceClient\n",
    "import io\n",
    "import pickle as pkl\n",
    "\n",
    "# Define the PlayerPredictionModel class (only the relevant parts for testing)\n",
    "class PlayerPredictionModel:\n",
    "    def __init__(self):\n",
    "        self.player_data = None\n",
    "        self.cols = None\n",
    "\n",
    "    def load_data(self):\n",
    "        workspace = WorkspaceClient()\n",
    "        \n",
    "        # Load columns from DBFS\n",
    "        try:\n",
    "            pkl_content = workspace.dbfs.download(\"dbfs:/FileStore/features.pkl\")\n",
    "            pkl_str = pkl_content.read()\n",
    "            self.cols = pkl.loads(pkl_str)\n",
    "            print(f\"Columns loaded successfully: {self.cols}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load column information: {str(e)}\")\n",
    "            raise RuntimeError(f\"Failed to load column information: {str(e)}\")\n",
    "        \n",
    "        # Load player data from DBFS\n",
    "        try:\n",
    "            file_content = workspace.dbfs.download(\"dbfs:/FileStore/inference_data.csv\")\n",
    "            file_str = file_content.read()\n",
    "            file_content_stream = io.BytesIO(file_str)\n",
    "            self.player_data = pd.read_csv(file_content_stream, encoding=\"latin1\")\n",
    "            \n",
    "            # Preprocess the entire dataset\n",
    "            self.player_data = self.player_data.sort_values(['Player', 'Year', 'G'], ascending=[True, False, False])\n",
    "            self.player_data = self.player_data.groupby(['Player', 'Year']).first().reset_index()\n",
    "            \n",
    "            print(f\"Player data loaded and preprocessed. Shape: {self.player_data.shape}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load or preprocess player data: {str(e)}\")\n",
    "            raise RuntimeError(f\"Failed to load or preprocess player data: {str(e)}\")\n",
    "\n",
    "    def calculate_player_averages(self, player_names):\n",
    "        \"\"\"\n",
    "        This function calculates the averages of NBA player statistics for one or more players,\n",
    "        including those with only one year of data.\n",
    "        \"\"\"\n",
    "        # Ensure player_names is a list\n",
    "        player_names = [player_names] if isinstance(player_names, str) else player_names\n",
    "\n",
    "        # Filter the preprocessed data for the requested players\n",
    "        player_data = self.player_data[self.player_data['Player'].isin(player_names)]\n",
    "\n",
    "        # Ensure all columns in self.cols are present in player_data\n",
    "        missing_cols = set(self.cols) - set(player_data.columns)\n",
    "        if missing_cols:\n",
    "            print(f\"Columns {missing_cols} not found in player data. They will be excluded.\")\n",
    "            cols_to_use = [col for col in self.cols if col in player_data.columns]\n",
    "        else:\n",
    "            cols_to_use = self.cols\n",
    "\n",
    "        def calc_player_stats(group):\n",
    "            # Sort by Year descending\n",
    "            sorted_group = group.sort_values('Year', ascending=False)\n",
    "            # Take up to 3 most recent years, or all available if less than 3\n",
    "            recent_years = sorted_group.iloc[:3]\n",
    "            # Calculate mean of available data\n",
    "            return recent_years[cols_to_use].mean()\n",
    "\n",
    "        # Apply the calculation to each player\n",
    "        player_averages = player_data.groupby('Player').apply(calc_player_stats).reset_index()\n",
    "\n",
    "        # Round numeric columns to 2 decimal places\n",
    "        player_averages[cols_to_use] = player_averages[cols_to_use].round(2)\n",
    "\n",
    "        # Check for truly missing players (not in the dataset at all)\n",
    "        missing_players = set(player_names) - set(player_averages['Player'])\n",
    "        if missing_players:\n",
    "            print(f\"No data found for players: {missing_players}\")\n",
    "            missing_df = pd.DataFrame({'Player': list(missing_players)})\n",
    "            player_averages = pd.concat([player_averages, missing_df], ignore_index=True)\n",
    "\n",
    "        print(f\"Calculated averages for {len(player_averages)} players\")\n",
    "        return player_averages\n",
    "\n",
    "# Create an instance of the model and load data\n",
    "model = PlayerPredictionModel()\n",
    "model.load_data()\n",
    "\n",
    "# Test cases\n",
    "test_cases = [\n",
    "    [\"LeBron James\"],  # Player with multiple years of data\n",
    "    [\"Chet Holmgren\"],  # Player with one year of data\n",
    "    [\"LeBron James\", \"Chet Holmgren\"],  # Mix of players with different amounts of data\n",
    "    [\"LeBron James\", \"Chet Holmgren\", \"NonexistentPlayer\"]  # Including a player not in the dataset\n",
    "]\n",
    "\n",
    "# Run tests\n",
    "for i, players in enumerate(test_cases, 1):\n",
    "    print(f\"\\nTest Case {i}: {players}\")\n",
    "    result = model.calculate_player_averages(players)\n",
    "    print(result)\n",
    "    print(\"\\nColumns in result:\")\n",
    "    print(result.columns.tolist())\n",
    "    print(\"\\nData types of columns:\")\n",
    "    print(result.dtypes)\n",
    "\n",
    "# Additional test: check for any NaN values in the result\n",
    "for i, players in enumerate(test_cases, 1):\n",
    "    print(f\"\\nChecking for NaN values in Test Case {i}: {players}\")\n",
    "    result = model.calculate_player_averages(players)\n",
    "    nan_columns = result.columns[result.isna().any()].tolist()\n",
    "    if nan_columns:\n",
    "        print(f\"Columns with NaN values: {nan_columns}\")\n",
    "    else:\n",
    "        print(\"No NaN values found in the result.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.models import validate_serving_input\n",
    "import mlflow\n",
    "from databricks.sdk import WorkspaceClient\n",
    "\n",
    "mlflow.set_tracking_uri(\"databricks\")\n",
    "\n",
    "model_uri = 'models:/NBA_XGB_Final/1'\n",
    "\n",
    "# The model is logged with an input example. MLflow converts\n",
    "# it into the serving payload format for the deployed model endpoint,\n",
    "# and saves it to 'serving_input_payload.json'\n",
    "serving_payload = \"\"\"{\n",
    "  \"dataframe_split\": {\n",
    "    \"columns\": [\n",
    "      \"Player\"\n",
    "    ],\n",
    "    \"data\": [\n",
    "      [\n",
    "        \"Emoni Bates\"\n",
    "      ],\n",
    "      [\n",
    "        \"Luke Hemmings\"\n",
    "      ]\n",
    "    ]\n",
    "  }\n",
    "}\"\"\"\n",
    "# Validate the serving payload works on the model\n",
    "validate_serving_input(model_uri, serving_payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model prediction result:\n",
      "{\n",
      "  \"predictions\": [\n",
      "    {\n",
      "      \"Player\": \"Klay Thompson\",\n",
      "      \"Predicted PPG\": \"19.86\"\n",
      "    },\n",
      "    {\n",
      "      \"Player\": \"Stephen Curry\",\n",
      "      \"Predicted PPG\": \"26.42\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "from databricks_cli.configure.provider import get_config\n",
    "from databricks_cli.configure.config import _get_api_client\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(\"C:\\\\hoops_ml\\\\.databricks\\\\.databricks.env\")\n",
    "token = os.getenv('DATABRICKS_TOKEN')\n",
    "\n",
    "def create_tf_serving_json(data):\n",
    "    return {'inputs': {name: data[name].tolist() for name in data.keys()} if isinstance(data, dict) else data.tolist()}\n",
    "\n",
    "def score_model(dataset):\n",
    "    url = 'https://2643df25324c4b119a31c6f0abed4f3f.8.serving.azuredatabricks.net/8066068091517004/serving-endpoints/nbapred/invocations'\n",
    "    headers = {'Authorization': f'Bearer {token}', 'Content-Type': 'application/json'}\n",
    "    ds_dict = {'dataframe_split': dataset.to_dict(orient='split')} if isinstance(dataset, pd.DataFrame) else create_tf_serving_json(dataset)\n",
    "    data_json = json.dumps(ds_dict, allow_nan=True)\n",
    "    response = requests.request(method='POST', headers=headers, url=url, data=data_json)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f'Request failed with status {response.status_code}, {response.text}')\n",
    "    return response.json()\n",
    "\n",
    "# Create the input data\n",
    "input_data = pd.DataFrame({\n",
    "    \"Player\": [\"Klay Thompson\", \"Stephen Curry\"]\n",
    "})\n",
    "\n",
    "# Call the score_model function\n",
    "try:\n",
    "    result = score_model(input_data)\n",
    "    print(\"Model prediction result:\")\n",
    "    print(json.dumps(result, indent=2))\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
